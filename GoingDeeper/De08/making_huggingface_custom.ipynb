{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9fd90b8-961d-40b6-bf04-b990eb30f284",
   "metadata": {},
   "source": [
    "## 프로젝트 목표\n",
    "---\n",
    "\n",
    "1. 모델과 데이터를 정상적으로 불러오고, 작동하는 것을 확인하였다.\n",
    " * klue/bert-base를 NSMC 데이터셋으로 fine-tuning 하여, 모델이 정상적으로 작동하는 것을 확인하였다.\n",
    "2. Preprocessing을 개선하고, fine-tuning을 통해 모델의 성능을 개선시켰다.\n",
    " * Validation accuracy를 90% 이상으로 개선하였다.\n",
    "3. 모델 학습에 Bucketing을 성공적으로 적용하고, 그 결과를 비교분석하였다.\n",
    " * Bucketing task을 수행하여 fine-tuning 시 연산 속도와 모델 성능 간의 trade-off 관계가 발생하는지 여부를 확인하고, 분석한 결과를 제시하였다.\n",
    "\n",
    "### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47ab1e24-bd86-4745-8213-78df59a0fb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 04:41:49.647582: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-03-19 04:41:49.647644: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba71859-6f7b-4626-bcec-4d3d9c518820",
   "metadata": {},
   "source": [
    "### STEP 1. NSMC 데이터 분석 및 Huggingface dataset 구성\n",
    "---\n",
    "huggingface nsmc dataset을 확인해보면 위와 같이 구성되어 있습니다.\n",
    "\n",
    "Dataset dictionary안에 train dataset, test dataset으로 구성되어 있고 각 Dataset은 ‘id’, ‘document’, ‘label’로 구성되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34d959da-6d08-41da-a5de-97519cdea9a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "huggingface_nsmc_dataset = load_dataset('nsmc')\n",
    "print(huggingface_nsmc_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d97e45f1-0923-4a4a-a5e2-e070cb01dc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'document', 'label']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = huggingface_nsmc_dataset['train']\n",
    "cols = train.column_names\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74745e13-1525-47e8-9395-dd1542ef996d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 9976970\n",
      "document : 아 더빙.. 진짜 짜증나네요 목소리\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 3819312\n",
      "document : 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "label : 1\n",
      "\n",
      "\n",
      "id : 10265843\n",
      "document : 너무재밓었다그래서보는것을추천한다\n",
      " : 0l\n",
      "\n",
      "\n",
      "id : 9045019\n",
      "document : 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 6483659\n",
      "document : 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
      "label : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e287bd87-73ba-4bdd-bb85-304f08249b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_nsmc_dataset = huggingface_nsmc_dataset.remove_columns([\"id\"])\n",
    "huggingface_nsmc_dataset = huggingface_nsmc_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78dfae69-70c3-4f7d-8bd3-a170cdf4e8a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['document', 'labels']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = huggingface_nsmc_dataset['train']\n",
    "cols = train.column_names\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e38d42-0d42-48a0-a4ee-b9b9f6eec21d",
   "metadata": {},
   "source": [
    "### STEP 2. klue/bert-base model 및 tokenizer 불러오기\n",
    "---\n",
    "* output_loading_info=True: To show detailed information about loaded and skipped weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13b57368-4c4f-4fa4-8cb2-42b1d5334b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights: ['classifier.bias', 'classifier.weight']\n",
      "Unused weights: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "Mismatched weights: []\n",
      "Error messages: []\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model, loading_info = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 2 , output_loading_info=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "# # Load the model with loading info\n",
    "# model, loading_info = AutoModelForMaskedLM.from_pretrained(\"klue/bert-base\", output_loading_info=True)\n",
    "\n",
    "# Inspect loaded weights\n",
    "print(\"Loaded weights:\", loading_info[\"missing_keys\"])\n",
    "\n",
    "# Inspect unused weights (weights in the checkpoint not used by the model)\n",
    "print(\"Unused weights:\", loading_info[\"unexpected_keys\"])\n",
    "\n",
    "# Inspect any mismatched weights\n",
    "print(\"Mismatched weights:\", loading_info[\"mismatched_keys\"])\n",
    "\n",
    "# Inspect any error messages during loading\n",
    "print(\"Error messages:\", loading_info[\"error_msgs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acbfbae-95ea-4b63-b373-e622df6204e9",
   "metadata": {},
   "source": [
    "### STEP 3. 위에서 불러온 tokenizer으로 데이터셋을 전처리하고, model 학습 진행해 보기\n",
    "---\n",
    "* MAX_LENGTH: 40\n",
    "* return_token_type_ids는 문장이 한개이상일 때 나뉘는걸 보여줍니다. (해당 내용은 task에 필요없으므로 제거합니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fba9befe-bbb1-4805-ba69-4107a894b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "def tokenize_function(data):\n",
    "    return tokenizer(data[\"document\"], truncation=True, padding=True, max_length=MAX_LENGTH, \n",
    "        return_token_type_ids = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1014994-9488-4bc4-b6c4-707f597159ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = huggingface_nsmc_dataset.map(tokenize_function, batched=True)\n",
    "train_dataset = tokenized_datasets['train']\n",
    "test_dataset = tokenized_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f027a4e3-c357-4aa7-b94d-041b565dd401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fb4d07d-6c94-465b-a16e-40ce43240fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': ['아 더빙.. 진짜 짜증나네요 목소리', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다'], 'labels': [0, 1, 0], 'input_ids': [[2, 1376, 831, 2604, 18, 18, 4229, 9801, 2075, 2203, 2182, 4243, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 1963, 18, 18, 18, 11811, 2178, 2088, 28883, 16516, 2776, 18, 18, 18, 18, 10737, 2156, 2015, 2446, 2232, 6758, 2118, 1380, 6074, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca5b91-962d-4f0b-b822-d4b25f5b533e",
   "metadata": {},
   "source": [
    "**Trainer를 활용한 학습**\n",
    "\n",
    "---\n",
    "Trainer를 사용하기 위해서는 TrainingArguments를 통해 학습 관련 설정을 미리 지정해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1adf0e80-8c9e-4a6d-9a39-a03ae350c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Training and Validation\n",
    "split_datasets = train_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Access the split datasets\n",
    "train_split = split_datasets[\"train\"]\n",
    "eval_split = split_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d43589e6-b81c-43f1-b7dc-7898b3381b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 4)\n",
      "(30000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train_split.shape)\n",
    "print(eval_split.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68ad6857-e3b9-401c-90ca-99e0d542a110",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir,                                         # output이 저장될 경로\n",
    "#     evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "#     learning_rate = 2e-5,                         #learning_rate\n",
    "#     per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "#     per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "#     num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "#     weight_decay = 0.01,                        # weight decay\n",
    "# )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir,                     # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",    #evaluation하는 빈도\n",
    "    learning_rate=2e-5,             #learning_rate\n",
    "    per_device_train_batch_size=16, # 각 device 당 batch size \n",
    "    per_device_eval_batch_size=64,  # evaluation 시에 batch size\n",
    "    num_train_epochs=3,             # train 시킬 총 epochs\n",
    "    warmup_steps=500,               # learning rate scheduler에 따른 warmup_step 설정  \n",
    "    do_train=True,                  # train 수행여부\n",
    "    do_eval=True,                   # eval 수행여부\n",
    "    eval_steps=1000,\n",
    "    group_by_length=False,    \n",
    "    weight_decay=0.01,              # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c46cac8d-3054-49bb-a57e-5f8c93336412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "metric = load('glue', 'mrpc')\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6cade2c-f34d-464d-98e7-6bf7eb048b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22500' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22500/22500 34:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.262700</td>\n",
       "      <td>0.257416</td>\n",
       "      <td>0.896133</td>\n",
       "      <td>0.896244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>0.301868</td>\n",
       "      <td>0.899133</td>\n",
       "      <td>0.899388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.126400</td>\n",
       "      <td>0.397808</td>\n",
       "      <td>0.900933</td>\n",
       "      <td>0.901144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22500, training_loss=0.21029578145345051, metrics={'train_runtime': 2085.4675, 'train_samples_per_second': 172.623, 'train_steps_per_second': 10.789, 'total_flos': 7399998432000000.0, 'train_loss': 0.21029578145345051, 'epoch': 3.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,           # 학습시킬 model\n",
    "    args=training_args,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=train_split,    # training dataset\n",
    "    eval_dataset=eval_split,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "767e9d3f-1aca-4cd1-9daa-a8a284ccdb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir + \"/manual_saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8db7424-d256-4712-8cbd-fb4ae02ddb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.39560624957084656,\n",
       " 'eval_accuracy': 0.90034,\n",
       " 'eval_f1': 0.9019306843006436,\n",
       " 'eval_runtime': 30.6661,\n",
       " 'eval_samples_per_second': 1630.465,\n",
       " 'eval_steps_per_second': 25.5,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2447a41-88d2-406f-89bc-0ca43dd355d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c5da0-93d4-44b4-8da8-b722c6747234",
   "metadata": {},
   "source": [
    "### STEP 4. Fine-tuning을 통하여 모델 성능(accuarcy) 향상시키기\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d3565-131b-4a96-bbaf-a0c7597d3cb4",
   "metadata": {},
   "source": [
    "### STEP 5. Bucketing을 적용하여 학습시키고, STEP 4의 결과와의 비교\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "597351b1-faa0-44bc-82b2-c0f6dab4b9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights: ['classifier.bias', 'classifier.weight']\n",
      "Unused weights: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "Mismatched weights: []\n",
      "Error messages: []\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model_name = \"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model, loading_info = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 2 , output_loading_info=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "# # Load the model with loading info\n",
    "# model, loading_info = AutoModelForMaskedLM.from_pretrained(\"klue/bert-base\", output_loading_info=True)\n",
    "\n",
    "# Inspect loaded weights\n",
    "print(\"Loaded weights:\", loading_info[\"missing_keys\"])\n",
    "\n",
    "# Inspect unused weights (weights in the checkpoint not used by the model)\n",
    "print(\"Unused weights:\", loading_info[\"unexpected_keys\"])\n",
    "\n",
    "# Inspect any mismatched weights\n",
    "print(\"Mismatched weights:\", loading_info[\"mismatched_keys\"])\n",
    "\n",
    "# Inspect any error messages during loading\n",
    "print(\"Error messages:\", loading_info[\"error_msgs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7e7e648f-8aa6-441c-94fd-70b6771e1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "def tokenize_bucket(data):\n",
    "    return tokenizer(data[\"document\"], truncation=True, padding=False, max_length=MAX_LENGTH, \n",
    "        return_token_type_ids = False)\n",
    "    \n",
    "tokenized_bucket_datasets = huggingface_nsmc_dataset.map(tokenize_bucket, batched=True)\n",
    "train_bucket_dataset = tokenized_bucket_datasets['train']\n",
    "test_bucket_dataset = tokenized_bucket_datasets['test']\n",
    "\n",
    "split_bucket_datasets = train_bucket_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Access the split datasets\n",
    "train_bucket_split = split_bucket_datasets[\"train\"]\n",
    "eval_bucket_plit = split_bucket_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "acb1330a-130a-44b4-9e39-7bc53fe5602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_bucket_args = TrainingArguments(\n",
    "    output_dir,                     # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",    #evaluation하는 빈도\n",
    "    learning_rate=2e-5,             #learning_rate\n",
    "    per_device_train_batch_size=16, # 각 device 당 batch size \n",
    "    per_device_eval_batch_size=64,  # evaluation 시에 batch size\n",
    "    num_train_epochs=3,             # train 시킬 총 epochs\n",
    "    group_by_length=True,    \n",
    "    weight_decay=0.01,              # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "696020c5-8538-495d-a720-dfcc9d0ad460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22500' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22500/22500 36:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>0.263224</td>\n",
       "      <td>0.895500</td>\n",
       "      <td>0.896237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.305870</td>\n",
       "      <td>0.898767</td>\n",
       "      <td>0.900109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.400213</td>\n",
       "      <td>0.899500</td>\n",
       "      <td>0.899904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22500, training_loss=0.20572020467122396, metrics={'train_runtime': 2169.6068, 'train_samples_per_second': 165.929, 'train_steps_per_second': 10.371, 'total_flos': 3740584096289280.0, 'train_loss': 0.20572020467122396, 'epoch': 3.0})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Pass the collator to the Trainer\n",
    "bucket_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_bucket_args,\n",
    "    train_dataset=train_bucket_split,\n",
    "    eval_dataset=eval_bucket_plit,\n",
    "    data_collator=data_collator,  # Dynamic padding applied here\n",
    "    compute_metrics=compute_metrics,  \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "bucket_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d57713d-b63d-4737-96f9-01b7600d71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_trainer.save_model(output_dir + \"/bucket_saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3cde8cf3-2100-4b5e-84a1-42aeedea3588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3911927044391632,\n",
       " 'eval_accuracy': 0.89982,\n",
       " 'eval_f1': 0.9012518482010843,\n",
       " 'eval_runtime': 29.9782,\n",
       " 'eval_samples_per_second': 1667.878,\n",
       " 'eval_steps_per_second': 26.086,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabe1c97-c40c-4b03-b32a-500a5baa4128",
   "metadata": {},
   "source": [
    "## 테스트 결과\n",
    "\n",
    "---\n",
    "|지표|NB|B|sst2|\n",
    "|:---|:---:|---:|---:|\n",
    "|train dataset size|120,000|120,000|120,000|\n",
    "|eval dataset size|30,000|30,000|30,000|\n",
    "|test dataset size|50,000|50,000|50,000|\n",
    "|global step|22,500|22,500|22,500|\n",
    "|훈련 시간|34:45|36:29|34:21|\n",
    "|훈련 손실|0.210|0.210|0.082|\n",
    "|훈련 정확도|0.901|0.900|0.897|\n",
    "|훈련 F1 점수|0.901|0.900|X|\n",
    "|훈련 실행 시간(초)|2,085|21,90|2,058|\n",
    "|초당 훈련 샘플 수|172.6|164.3|174.8|\n",
    "|초당 훈련 단계 수|10.78|10.27|10.93|\n",
    "|테스트 시간|00:30|00:29|00:29|\n",
    "|테스트 손실|0.395|0.391|0.648|\n",
    "|테스트 정확도|0.900|0.900|0.896|\n",
    "|테스트 F1 점수|0.902|0.901|X|\n",
    "|패딩 방식|40 고정|동적 길이|40 고정|\n",
    "1. ***최초 실행시 lecture의 Arguments 상향 하여 90% 이상 Accuracy, F1 달성***\n",
    "   * Arguments 변경: per_device_train_batch_size = 8 --> 16, per_device_eval_batch_size = 8 --> 64\n",
    "   * tokenizer's MAX_LENGTH = 40 ==> 이전 프로젝트에서 사용된 네이버 영화 감성 분석 데이터의 분포를 참조.\n",
    "  ![mrpc_result](./mrpc_result.png)\n",
    "\n",
    "2. ***Bucketing 결과***\n",
    "    * 소요 시간 단축을 예상 했으나 조금 더 걸림(36:29, 34:45(not bucketing))\n",
    "    * Accuracy, F1 score도 낮음\n",
    "![mrpc_bucketing_result](./mrpc_bucketing_result.png)\n",
    "\n",
    "3. ***동일한 조건에서 메트릭 변경 (mprc-->sst2)***\n",
    "    * 정확도가 조금 떨어짐.\n",
    "![sst2_first_result](./sst2_first_result.png)\n",
    "![sst2_test_first_result](./sst2_test_first_result.png)\n",
    "\n",
    "4. ***실행 후 tokenizer config 파일을 통해 확인 한 정보***\n",
    "    * vocab size:32000, tensor type:pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf42ff-caf2-4177-a732-7ef8176b3d2c",
   "metadata": {},
   "source": [
    "## 회고\n",
    " * 평가지표는 감정분석(sst2)보다 두 문장의 유사도(mprc)의 정확도가 높은 것은\n",
    "   생성한 클래스가 AutoModelForSequenceClassification이고 입력 파라미터 중 이진 분류 파라미터를 설정했기 때문으로 유추 하고 있으나 더 깊은 학습 필요 함.\n",
    " * 모델을 삭제 하지 않고 다른 파라미터를 조정하여 진행하니 train loss는 계속 감소함. 정확한 진행을 위해 지표 변경 후 실행 시 모델 삭제 후 재 생성 하는 것이 좋음.\n",
    " * Bucketing 결과를 보면 무언가 적용이 잘 안 된 것 같아 효과가 나타나지 않은 것 같음. 관련 예제나 API 분석이 필요 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c405b5eb-daa3-4661-a155-a636fd0bc269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
